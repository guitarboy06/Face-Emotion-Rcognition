{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'disgust', 'fear', 'happy', 'nutral', 'sad', 'surprise']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories=os.listdir('C:/Users/hp/face expression recognition/data/train')\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=cv2.imread('C:/Users/hp/face expression recognition/data/train/angry/5132.png')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=[]\n",
    "train_label=[]\n",
    "for cat in categories:\n",
    "    train_img_path=os.path.join('C:/Users/hp/face expression recognition/data/train',cat)\n",
    "    image_name=os.listdir(train_img_path)\n",
    "    \n",
    "    for image in image_name:\n",
    "        img=cv2.imread(os.path.join(train_img_path,image))\n",
    "        train_image.append(img)\n",
    "        train_label.append(categories.index(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image[28000].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image=[]\n",
    "test_label=[]\n",
    "for cat in categories:\n",
    "    test_img_path=os.path.join('C:/Users/hp/face expression recognition/data/test',cat)\n",
    "    image_name=os.listdir(test_img_path)\n",
    "    \n",
    "    for image in image_name:\n",
    "        img=cv2.imread(os.path.join(test_img_path,image))\n",
    "        test_image.append(img)\n",
    "        test_label.append(categories.index(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image=[]\n",
    "val_label=[]\n",
    "for cat in categories:\n",
    "    val_img_path=os.path.join('C:/Users/hp/face expression recognition/data/val',cat)\n",
    "    image_name=os.listdir(val_img_path)\n",
    "    \n",
    "    for image in image_name:\n",
    "        img=cv2.imread(os.path.join(val_img_path,image))\n",
    "        val_image.append(img)\n",
    "        val_label.append(categories.index(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img=np.reshape(train_image,(len(train_image),48,48,3))\n",
    "test_img=np.reshape(test_image,(len(test_image),48,48,3))\n",
    "val_img=np.reshape(val_image,(len(val_image),48,48,3))\n",
    "train_lab=np.reshape(train_label,(len(train_label),))\n",
    "test_lab=np.reshape(test_label,(len(test_label),))\n",
    "val_lab=np.reshape(val_label,(len(val_label),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img,train_lab=shuffle(train_img,train_lab)\n",
    "test_img,test_lab=shuffle(test_img,test_lab)\n",
    "val_img,val_lab=shuffle(val_img,val_lab)\n",
    "train_lab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "train_lab=to_categorical(train_lab,num_classes=7,dtype='int')\n",
    "test_lab=to_categorical(test_lab,num_classes=7,dtype='int')\n",
    "val_lab=to_categorical(val_lab,num_classes=7,dtype='int')\n",
    "train_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dense,Dropout,Input,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 48, 48, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 46, 46, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 19, 19, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 56)                580664    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 28)                1596      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 203       \n",
      "=================================================================\n",
      "Total params: 843,391\n",
      "Trainable params: 843,007\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "expression=Sequential()\n",
    "expression.add(Input(shape=(48,48,3)))\n",
    "\n",
    "expression.add(Conv2D(64,3,activation='relu',padding='same'))\n",
    "expression.add(BatchNormalization())\n",
    "expression.add(Conv2D(64,3,activation='relu'))\n",
    "expression.add(MaxPooling2D(pool_size=2))\n",
    "expression.add(Dropout(0.2))\n",
    "\n",
    "expression.add(Conv2D(128,3,activation='relu'))\n",
    "expression.add(BatchNormalization())\n",
    "expression.add(Conv2D(128,3,activation='relu'))\n",
    "expression.add(MaxPooling2D(pool_size=2))\n",
    "expression.add(Dropout(0.2))\n",
    "\n",
    "expression.add(Flatten())\n",
    "expression.add(Dense(56,activation='relu'))\n",
    "expression.add(Dense(28,activation='relu'))\n",
    "expression.add(Dense(7,activation='softmax'))\n",
    "\n",
    "expression.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "expression.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=ModelCheckpoint('model-{epoch:03d}.model',monitor='val_accuracy',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.6739 - accuracy: 0.3201WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "898/898 [==============================] - 416s 463ms/step - loss: 1.6739 - accuracy: 0.3201 - val_loss: 1.5452 - val_accuracy: 0.3734\n",
      "Epoch 2/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.4207 - accuracy: 0.4206INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "898/898 [==============================] - 418s 465ms/step - loss: 1.4207 - accuracy: 0.4206 - val_loss: 1.4813 - val_accuracy: 0.3920\n",
      "Epoch 3/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.2999 - accuracy: 0.4823INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "898/898 [==============================] - 484s 539ms/step - loss: 1.2999 - accuracy: 0.4823 - val_loss: 1.2980 - val_accuracy: 0.4795\n",
      "Epoch 4/30\n",
      "898/898 [==============================] - 483s 538ms/step - loss: 1.1888 - accuracy: 0.5292 - val_loss: 1.4312 - val_accuracy: 0.4408\n",
      "Epoch 5/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.0951 - accuracy: 0.5750INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "898/898 [==============================] - 437s 486ms/step - loss: 1.0951 - accuracy: 0.5750 - val_loss: 1.3106 - val_accuracy: 0.5001\n",
      "Epoch 6/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 1.0065 - accuracy: 0.6110INFO:tensorflow:Assets written to: model-006.model\\assets\n",
      "898/898 [==============================] - 407s 453ms/step - loss: 1.0065 - accuracy: 0.6110 - val_loss: 1.2187 - val_accuracy: 0.5525\n",
      "Epoch 7/30\n",
      "898/898 [==============================] - 405s 451ms/step - loss: 0.9119 - accuracy: 0.6555 - val_loss: 1.2185 - val_accuracy: 0.5517\n",
      "Epoch 8/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 0.8227 - accuracy: 0.6928INFO:tensorflow:Assets written to: model-008.model\\assets\n",
      "898/898 [==============================] - 412s 459ms/step - loss: 0.8227 - accuracy: 0.6928 - val_loss: 1.1869 - val_accuracy: 0.5743\n",
      "Epoch 9/30\n",
      "898/898 [==============================] - 415s 462ms/step - loss: 0.7527 - accuracy: 0.7184 - val_loss: 1.3088 - val_accuracy: 0.5670\n",
      "Epoch 10/30\n",
      "898/898 [==============================] - 421s 469ms/step - loss: 0.6820 - accuracy: 0.7481 - val_loss: 1.3193 - val_accuracy: 0.5673\n",
      "Epoch 11/30\n",
      "898/898 [==============================] - 418s 465ms/step - loss: 0.6202 - accuracy: 0.7723 - val_loss: 1.3414 - val_accuracy: 0.5734\n",
      "Epoch 12/30\n",
      "898/898 [==============================] - 422s 470ms/step - loss: 0.5644 - accuracy: 0.7934 - val_loss: 1.4196 - val_accuracy: 0.5726\n",
      "Epoch 13/30\n",
      "898/898 [==============================] - 450s 501ms/step - loss: 0.5169 - accuracy: 0.8111 - val_loss: 1.5521 - val_accuracy: 0.5743\n",
      "Epoch 14/30\n",
      "898/898 [==============================] - 502s 560ms/step - loss: 0.4828 - accuracy: 0.8235 - val_loss: 1.5354 - val_accuracy: 0.5681\n",
      "Epoch 15/30\n",
      "898/898 [==============================] - 487s 542ms/step - loss: 0.4471 - accuracy: 0.8380 - val_loss: 1.5460 - val_accuracy: 0.5734\n",
      "Epoch 16/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 0.4120 - accuracy: 0.8525INFO:tensorflow:Assets written to: model-016.model\\assets\n",
      "898/898 [==============================] - 832s 927ms/step - loss: 0.4120 - accuracy: 0.8525 - val_loss: 1.6190 - val_accuracy: 0.5798\n",
      "Epoch 17/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 0.3837 - accuracy: 0.8638INFO:tensorflow:Assets written to: model-017.model\\assets\n",
      "898/898 [==============================] - 501s 558ms/step - loss: 0.3837 - accuracy: 0.8638 - val_loss: 1.6025 - val_accuracy: 0.5815\n",
      "Epoch 18/30\n",
      "898/898 [==============================] - 504s 561ms/step - loss: 0.3645 - accuracy: 0.8688 - val_loss: 1.6642 - val_accuracy: 0.5626\n",
      "Epoch 19/30\n",
      "898/898 [==============================] - ETA: 0s - loss: 0.3453 - accuracy: 0.8753INFO:tensorflow:Assets written to: model-019.model\\assets\n",
      "898/898 [==============================] - 504s 561ms/step - loss: 0.3453 - accuracy: 0.8753 - val_loss: 1.7314 - val_accuracy: 0.5832\n",
      "Epoch 20/30\n",
      "898/898 [==============================] - 429s 477ms/step - loss: 0.3299 - accuracy: 0.8821 - val_loss: 1.7613 - val_accuracy: 0.5704\n",
      "Epoch 21/30\n",
      "898/898 [==============================] - 405s 451ms/step - loss: 0.3133 - accuracy: 0.8879 - val_loss: 1.7688 - val_accuracy: 0.5793\n",
      "Epoch 22/30\n",
      "898/898 [==============================] - 411s 458ms/step - loss: 0.2973 - accuracy: 0.8944 - val_loss: 1.7462 - val_accuracy: 0.5717\n",
      "Epoch 23/30\n",
      "898/898 [==============================] - 405s 451ms/step - loss: 0.2821 - accuracy: 0.8999 - val_loss: 1.8723 - val_accuracy: 0.5790\n",
      "Epoch 24/30\n",
      "898/898 [==============================] - 403s 449ms/step - loss: 0.2698 - accuracy: 0.9046 - val_loss: 1.9454 - val_accuracy: 0.5743\n",
      "Epoch 25/30\n",
      "898/898 [==============================] - 403s 449ms/step - loss: 0.2575 - accuracy: 0.9089 - val_loss: 1.9452 - val_accuracy: 0.5743\n",
      "Epoch 26/30\n",
      "898/898 [==============================] - 405s 451ms/step - loss: 0.2560 - accuracy: 0.9099 - val_loss: 1.8254 - val_accuracy: 0.5801\n",
      "Epoch 27/30\n",
      "898/898 [==============================] - 396s 441ms/step - loss: 0.2467 - accuracy: 0.9138 - val_loss: 1.9220 - val_accuracy: 0.5821\n",
      "Epoch 28/30\n",
      "898/898 [==============================] - 389s 433ms/step - loss: 0.2267 - accuracy: 0.9186 - val_loss: 1.9836 - val_accuracy: 0.5804\n",
      "Epoch 29/30\n",
      "898/898 [==============================] - 1411s 2s/step - loss: 0.2238 - accuracy: 0.9211 - val_loss: 1.9628 - val_accuracy: 0.5770\n",
      "Epoch 30/30\n",
      "898/898 [==============================] - 397s 442ms/step - loss: 0.2146 - accuracy: 0.9254 - val_loss: 2.0865 - val_accuracy: 0.5779\n"
     ]
    }
   ],
   "source": [
    "history=expression.fit(train_img,train_lab,validation_data=(val_img,val_lab),epochs=30,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 7s 66ms/step - loss: 1.9799 - accuracy: 0.5924\n"
     ]
    }
   ],
   "source": [
    "_,accuracy=expression.evaluate(test_img,test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
